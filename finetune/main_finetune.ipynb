{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "297f6078-0f90-4c3b-92d7-c84428fe6ba8",
   "metadata": {},
   "source": [
    "We start by installing all the packages that are needed for the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98091ffd-b36f-459b-b344-f2dffe18cc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qqq -U torch transformers datasets evaluate accelerate peft trl langchain bitsandbytes tensorboard python-dotenv --progress-bar off"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc6e384-ca74-465f-95e2-283397462739",
   "metadata": {},
   "source": [
    "And import all the packages and functions that are needed. We will do the fine-tuning using the HuggingFace packages that greatly simplifies the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffef89be-1a84-4370-b1f9-72f204c133c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import (\n",
    "    LoraConfig, \n",
    "    PeftModel,\n",
    ")\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "from evaluate import load\n",
    "from accelerate import PartialState\n",
    "import langchain\n",
    "from langchain.cache import SQLiteCache\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c057bf-e000-4c08-bd88-51db948440b7",
   "metadata": {},
   "source": [
    "It is important to include the HF_token in the *.env* file. By the time this notebook is created, the model that we are going to fine-tune (Llama3-8B) is only available after request for access. The HF_token identifies the user, making available the model for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2248c01-2cb4-4e34-bcc9-6772832b4c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(\".env\", override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10a8e10a-48aa-4bcb-88be-e27478dc5c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "base_model = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# Name of the new model\n",
    "new_model = \"TunedLlama-3-8B\"\n",
    "\n",
    "# Dataset\n",
    "dataset_path = \"LLM_organic_synthesis/workplace_data/datasets/USPTO-n100k-t2048_exp1/train.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74d8128c-0f3a-4057-85b9-192fdaa10d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_string = PartialState().process_index\n",
    "\n",
    "compute_dtype = getattr(torch, \"float16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83fe6547-e545-4921-a857-88efc9569795",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"json\", data_files=dataset_path, split=\"train\")\n",
    "dataset = dataset.shuffle(seed=42).select(range(1000)) # Only use 10000 samples for quick demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5155090e-f419-4462-a736-eac0fc2d4da6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'output'],\n",
       "        num_rows: 990\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['instruction', 'output'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.01, seed=42)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56084e92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': \"Below is a description of an organic reaction. Extract information from it to an ORD JSON record.\\n\\n### Procedure:\\n110 mg of (R)-α-lipoic acid were dissolved in 2 ml of anhydrous dimethylformamide, and 97 mg of N,N'-carbonyldiimidazole were added to the solution, whilst ice-cooling. The mixture was then stirred at room temperature for 4 hours. At the end of this time, 57 mg of methanesulfonamide and 26 mg of sodium hydride (as a 55% w/w dispersion in mineral oil) were added to the reaction mixture, whilst ice-cooling, and the mixture was stirred at room temperature for 5 hours and then left to stand overnight. The solvent was then removed from the reaction mixture by evaporation under reduced pressure, and water was added to the residue this obtained. The resulting mixture was neutralized by the addition of 2 N aqueous hydrochloric acid, after which it was extracted with ethyl acetate. The extraction solution was washed with a saturated aqueous solution of sodium chloride and then dried over anhydrous sodium sulfate. The solvent was removed from the extraction solution by evaporation under reduced pressure. The residue thus obtained was purified by silica gel column chromatography, using 1:1 and 3:1 by volume mixtures of ethyl acetate and hexane as eluent, after which it was dissolved in dioxane and lyophilised, to obtain 68 mg of the title compound, melting at 71° C. to 73° C.\\n\\n### ORD JSON:\\n\",\n",
       " 'output': '{\"inputs\": {\"m2\": {\"components\": [{\"identifiers\": [{\"type\": \"NAME\", \"value\": \"N,N\\'-carbonyldiimidazole\"}], \"amount\": {\"mass\": {\"value\": 97.0, \"units\": \"MILLIGRAM\"}}, \"reaction_role\": \"REACTANT\"}]}, \"m1_m5\": {\"components\": [{\"identifiers\": [{\"type\": \"NAME\", \"value\": \"(R)-\\\\u03b1-lipoic acid\"}], \"amount\": {\"mass\": {\"value\": 110.0, \"units\": \"MILLIGRAM\"}}, \"reaction_role\": \"REACTANT\"}, {\"identifiers\": [{\"type\": \"NAME\", \"value\": \"dimethylformamide\"}], \"amount\": {\"volume\": {\"value\": 2.0, \"units\": \"MILLILITER\"}}, \"reaction_role\": \"SOLVENT\"}]}, \"m3_m4\": {\"components\": [{\"identifiers\": [{\"type\": \"NAME\", \"value\": \"methanesulfonamide\"}], \"amount\": {\"mass\": {\"value\": 57.0, \"units\": \"MILLIGRAM\"}}, \"reaction_role\": \"REACTANT\"}, {\"identifiers\": [{\"type\": \"NAME\", \"value\": \"sodium hydride\"}], \"amount\": {\"mass\": {\"value\": 26.0, \"units\": \"MILLIGRAM\"}}, \"reaction_role\": \"REACTANT\"}]}}, \"conditions\": {\"temperature\": {\"control\": {\"type\": \"AMBIENT\"}}, \"stirring\": {\"type\": \"CUSTOM\", \"details\": \"The mixture was then stirred at room temperature for 4 hours\"}, \"conditions_are_dynamic\": true}, \"workups\": [{\"type\": \"TEMPERATURE\", \"details\": \"cooling\"}, {\"type\": \"TEMPERATURE\", \"details\": \"cooling\"}, {\"type\": \"STIRRING\", \"details\": \"the mixture was stirred at room temperature for 5 hours\", \"duration\": {\"value\": 5.0, \"units\": \"HOUR\"}, \"temperature\": {\"control\": {\"type\": \"AMBIENT\"}}}, {\"type\": \"WAIT\", \"details\": \"left\"}, {\"type\": \"WAIT\", \"details\": \"to stand overnight\", \"duration\": {\"value\": 8.0, \"precision\": 8.0, \"units\": \"HOUR\"}}, {\"type\": \"CUSTOM\", \"details\": \"The solvent was then removed from the reaction mixture by evaporation under reduced pressure, and water\", \"input\": {\"components\": [{\"identifiers\": [{\"type\": \"NAME\", \"value\": \"water\"}], \"reaction_role\": \"WORKUP\"}]}}, {\"type\": \"ADDITION\", \"details\": \"was added to the residue this obtained\"}, {\"type\": \"ADDITION\", \"details\": \"The resulting mixture was neutralized by the addition of 2 N aqueous hydrochloric acid\", \"input\": {\"components\": [{\"identifiers\": [{\"type\": \"NAME\", \"value\": \"hydrochloric acid\"}], \"reaction_role\": \"WORKUP\"}]}}, {\"type\": \"EXTRACTION\", \"details\": \"after which it was extracted with ethyl acetate\", \"input\": {\"components\": [{\"identifiers\": [{\"type\": \"NAME\", \"value\": \"ethyl acetate\"}], \"reaction_role\": \"WORKUP\"}]}}, {\"type\": \"WASH\", \"details\": \"The extraction solution was washed with a saturated aqueous solution of sodium chloride\", \"input\": {\"components\": [{\"identifiers\": [{\"type\": \"NAME\", \"value\": \"sodium chloride\"}], \"reaction_role\": \"WORKUP\"}]}}, {\"type\": \"DRY_WITH_MATERIAL\", \"details\": \"dried over anhydrous sodium sulfate\", \"input\": {\"components\": [{\"identifiers\": [{\"type\": \"NAME\", \"value\": \"sodium sulfate\"}], \"reaction_role\": \"WORKUP\"}]}}, {\"type\": \"CUSTOM\", \"details\": \"The solvent was removed from the extraction solution by evaporation under reduced pressure\"}, {\"type\": \"CUSTOM\", \"details\": \"The residue thus obtained\"}, {\"type\": \"CUSTOM\", \"details\": \"was purified by silica gel column chromatography\"}, {\"type\": \"DISSOLUTION\", \"details\": \"after which it was dissolved in dioxane\", \"input\": {\"components\": [{\"identifiers\": [{\"type\": \"NAME\", \"value\": \"dioxane\"}], \"reaction_role\": \"WORKUP\"}]}}], \"outcomes\": [{\"reaction_time\": {\"value\": 4.0, \"units\": \"HOUR\"}, \"products\": [{\"identifiers\": [{\"type\": \"NAME\", \"value\": \"title compound\"}], \"measurements\": [{\"type\": \"AMOUNT\", \"details\": \"MASS\", \"amount\": {\"mass\": {\"value\": 68.0, \"units\": \"MILLIGRAM\"}}}], \"reaction_role\": \"PRODUCT\"}]}]}'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a155007-573c-47f7-a8cc-3a3d136b5b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QLoRA config\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "# Compute dtype for 4-bit base models\n",
    "# Quantization type (fp4 or nf4)\n",
    "# Activate nested quantization for 4-bit base models\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# LoRA config\n",
    "peft_config = LoraConfig(\n",
    "    r=64, # The rank of the update matrices, expressed in int. Lower rank results in smaller update matrices with fewer trainable parameters.\n",
    "    lora_alpha=16, # LoRA scaling factor. It changes how the adaptation layer's weights affect the base model's\n",
    "    lora_dropout=0.1, # Dropout is a regularization technique where a proportion of neurons (or parameters) are randomly “dropped out” or turned off during training to prevent overfitting.\n",
    "    bias=\"none\", # Specifies if the bias parameters should be trained. Can be 'none', 'all' or 'lora_only'.\n",
    "    task_type=\"CAUSAL_LM\", # Task to perform, Causal LM: Causal language modeling.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ce4ca6d-883d-4a16-9c07-8ac2fd0b3abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ce47lin/miniconda3/envs/review/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6545f9081f64e4eb1052b396abea86d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Model config\n",
    "# Where the model is placed,set device_map=\"auto\" loads a model onto multiple GPUs..\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bf934d-c277-4857-83bc-925a28109fa0",
   "metadata": {},
   "source": [
    "Note the special characters that are introduced. They are the same that Meta used for the pre-training of the model. They are used to define to the model the different roles and the instructions statements of each role. They can be consulted here: https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3/ (When consulting the link care about both models, Llama 3 and Llama 3 Instruct)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edc3c7bd-5809-4e4d-a974-5ac8041d3daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(prompt):\n",
    "    return f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>{prompt['instruction']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>{prompt['output']}<|eot_id|>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60a515ac-8b34-46e9-ba2d-3ff777ec826c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    num_train_epochs=2,\n",
    "    fp16=False,\n",
    "    bf16=True, #bf16 to True with an A100\n",
    "    logging_steps=1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=0.5,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_steps=10,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    report_to=\"tensorboard\",\n",
    "    output_dir=\"./results/\",\n",
    "    save_strategy='no', # Only safe the final model, not the checkpoints\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e31e0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ce47lin/miniconda3/envs/review/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:246: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74a829b48f5a42f7b3a55e789a82f51e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f43b7f9bab1d4d36918dc741fbbce82a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    max_seq_length=None,\n",
    "    args=training_arguments,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,# Because the dataset already has the format {\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n",
    "    formatting_func=formatting_prompts_func,\n",
    "    # dataset_text_field='instruction',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31e1e64f-8e24-4478-abbd-4c6f0bae3cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='882' max='882' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [882/882 10:43, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>441</td>\n",
       "      <td>0.423100</td>\n",
       "      <td>0.582815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>882</td>\n",
       "      <td>0.486100</td>\n",
       "      <td>0.574735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=882, training_loss=0.5061882249757547, metrics={'train_runtime': 645.9727, 'train_samples_per_second': 2.731, 'train_steps_per_second': 1.365, 'total_flos': 8.163397024402637e+16, 'train_loss': 0.5061882249757547, 'epoch': 2.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fe16068-e588-40f9-8aca-a6a66ae17d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ce47lin/miniconda3/envs/review/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('final_checkpoint/tokenizer_config.json',\n",
       " 'final_checkpoint/special_tokens_map.json',\n",
       " 'final_checkpoint/tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model('final_checkpoint')\n",
    "tokenizer.save_pretrained('final_checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e12cc7c-b1c0-442d-9e2c-2f54eb3af53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flush memory\n",
    "del trainer, model\n",
    "gc.collect()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c38eaf24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ce46f10220b4923a2520cbb06bd08ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ce47lin/miniconda3/envs/review/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Reload tokenizer and model\n",
    "llama_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a27be4d-6347-4467-b29f-362b96bf4013",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ce47lin/miniconda3/envs/review/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('TunedLlama-3-8B/tokenizer_config.json',\n",
       " 'TunedLlama-3-8B/special_tokens_map.json',\n",
       " 'TunedLlama-3-8B/tokenizer.json')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge adapter with base model\n",
    "sft_model = PeftModel.from_pretrained(llama_model, 'final_checkpoint')\n",
    "sft_model = sft_model.merge_and_unload()\n",
    "\n",
    "# Save model and tokenizer\n",
    "sft_model.save_pretrained(new_model)\n",
    "tokenizer.save_pretrained(new_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
