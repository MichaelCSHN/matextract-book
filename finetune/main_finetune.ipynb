{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "297f6078-0f90-4c3b-92d7-c84428fe6ba8",
   "metadata": {},
   "source": [
    "We start by installing all the packages that are needed for the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98091ffd-b36f-459b-b344-f2dffe18cc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qqq -U torch transformers datasets evaluate accelerate peft trl langchain bitsandbytes tensorboard python-dotenv wandb --progress-bar off"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc6e384-ca74-465f-95e2-283397462739",
   "metadata": {},
   "source": [
    "And import all the packages and functions that are needed. We will do the fine-tuning using the HuggingFace packages that greatly simplifies the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffef89be-1a84-4370-b1f9-72f204c133c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import json\n",
    "import wandb\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from peft import (\n",
    "    LoraConfig, \n",
    "    PeftModel,\n",
    ")\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "from evaluate import load\n",
    "from accelerate import PartialState\n",
    "import langchain\n",
    "from langchain.cache import SQLiteCache\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c057bf-e000-4c08-bd88-51db948440b7",
   "metadata": {},
   "source": [
    "It is important to include the HF_token in the *.env* file. By the time this notebook is created, the model that we are going to fine-tune (Llama3-8B) is only available after request for access. The HF_token identifies the user, making available the model for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2248c01-2cb4-4e34-bcc9-6772832b4c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(\".env\", override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2814123e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmartinriosgarcia\u001b[0m (\u001b[33mprem-incar\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10a8e10a-48aa-4bcb-88be-e27478dc5c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "base_model = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# Name of the new model\n",
    "new_model = \"TunedLlama-3-8B\"\n",
    "\n",
    "# Dataset\n",
    "dataset_path = \"LLM_organic_synthesis/workplace_data/datasets/USPTO-n100k-t2048_exp1/train.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74d8128c-0f3a-4057-85b9-192fdaa10d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_string = PartialState().process_index\n",
    "\n",
    "compute_dtype = getattr(torch, \"float16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83fe6547-e545-4921-a857-88efc9569795",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"json\", data_files=dataset_path, split=\"train\")\n",
    "dataset = dataset.shuffle(seed=42).select(range(100)) # Only use 10000 samples for quick demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5155090e-f419-4462-a736-eac0fc2d4da6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'output'],\n",
       "        num_rows: 90\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['instruction', 'output'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56084e92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Below is a description of an organic reaction. Extract information from it to an ORD JSON record.\\n\\n### Procedure:\\n0.007 ml of propane-2-sulphonyl chloride is added to a solution of 0.026 g of tert-butyl {1-(2-amino -1-hydroxyethyl)-3-[3-(3-methoxypropyl)-1-methyl-1H-indol-5-ylmethyl]-4-methylpentyl}carbamate (Example 3Kb), and 0.007 ml of triethylamine in 1 ml of dichloromethane is added at 0° C. After 6 hours, the reaction mixture is concentrated by evaporation—the N-Boc intermediate is identified on the basis of the Rf value from the residue by means of flash chromatography (SiO2 60F). The N-Boc intermediate is dissolved in 0.82 ml of 4N HCl/dioxane—after 4 hours, the reaction mixture is concentrated by evaporation, and the residue is dissolved in 0.5 ml of tert-butanol, frozen in liquid nitrogen and lyophilized under high vacuum overnight. The title compound is identified on the basis of the Rf value from the residue.\\n\\n### ORD JSON:\\n',\n",
       " 'output': '{\"inputs\": {\"m4\": {\"components\": [{\"identifiers\": [{\"type\": \"NAME\", \"value\": \"dichloromethane\"}], \"amount\": {\"volume\": {\"value\": 1.0, \"units\": \"MILLILITER\"}}, \"reaction_role\": \"SOLVENT\"}]}, \"m1\": {\"components\": [{\"identifiers\": [{\"type\": \"NAME\", \"value\": \"propane-2-sulphonyl chloride\"}], \"amount\": {\"volume\": {\"value\": 0.007, \"units\": \"MILLILITER\"}}, \"reaction_role\": \"REACTANT\"}]}, \"m2\": {\"components\": [{\"identifiers\": [{\"type\": \"NAME\", \"value\": \"tert-butyl {1-(2-amino -1-hydroxyethyl)-3-[3-(3-methoxypropyl)-1-methyl-1H-indol-5-ylmethyl]-4-methylpentyl}carbamate\"}], \"amount\": {\"mass\": {\"value\": 0.026, \"units\": \"GRAM\"}}, \"reaction_role\": \"REACTANT\"}]}, \"m3\": {\"components\": [{\"identifiers\": [{\"type\": \"NAME\", \"value\": \"triethylamine\"}], \"amount\": {\"volume\": {\"value\": 0.007, \"units\": \"MILLILITER\"}}, \"reaction_role\": \"REACTANT\"}]}}, \"conditions\": {\"conditions_are_dynamic\": true}, \"workups\": [{\"type\": \"ADDITION\", \"details\": \"is added at 0\\\\u00b0 C\", \"temperature\": {\"setpoint\": {\"value\": 0.0, \"units\": \"CELSIUS\"}}}, {\"type\": \"CONCENTRATION\", \"details\": \"the reaction mixture is concentrated by evaporation\\\\u2014the N-Boc intermediate\", \"input\": {\"components\": [{\"identifiers\": [{\"type\": \"NAME\", \"value\": \"N-Boc\"}], \"reaction_role\": \"WORKUP\"}]}}, {\"type\": \"DISSOLUTION\", \"details\": \"The N-Boc intermediate is dissolved in 0.82 ml of 4N HCl/dioxane\\\\u2014after 4 hours\", \"duration\": {\"value\": 4.0, \"units\": \"HOUR\"}, \"input\": {\"components\": [{\"identifiers\": [{\"type\": \"NAME\", \"value\": \"N-Boc\"}], \"reaction_role\": \"WORKUP\"}, {\"identifiers\": [{\"type\": \"NAME\", \"value\": \"HCl dioxane\"}], \"amount\": {\"volume\": {\"value\": 0.82, \"units\": \"MILLILITER\"}}, \"reaction_role\": \"WORKUP\"}]}}, {\"type\": \"CONCENTRATION\", \"details\": \"the reaction mixture is concentrated by evaporation\"}, {\"type\": \"DISSOLUTION\", \"details\": \"the residue is dissolved in 0.5 ml of tert-butanol\", \"input\": {\"components\": [{\"identifiers\": [{\"type\": \"NAME\", \"value\": \"tert-butanol\"}], \"amount\": {\"volume\": {\"value\": 0.5, \"units\": \"MILLILITER\"}}, \"reaction_role\": \"WORKUP\"}]}}, {\"type\": \"TEMPERATURE\", \"details\": \"frozen in liquid nitrogen\"}, {\"type\": \"CUSTOM\", \"details\": \"lyophilized under high vacuum overnight\", \"duration\": {\"value\": 8.0, \"precision\": 8.0, \"units\": \"HOUR\"}}], \"outcomes\": [{\"reaction_time\": {\"value\": 6.0, \"units\": \"HOUR\"}, \"products\": [{}]}]}'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a155007-573c-47f7-a8cc-3a3d136b5b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QLoRA config\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "# Compute dtype for 4-bit base models\n",
    "# Quantization type (fp4 or nf4)\n",
    "# Activate nested quantization for 4-bit base models\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# LoRA config\n",
    "peft_config = LoraConfig(\n",
    "    r=64, # The rank of the update matrices, expressed in int. Lower rank results in smaller update matrices with fewer trainable parameters.\n",
    "    lora_alpha=16, # LoRA scaling factor. It changes how the adaptation layer's weights affect the base model's\n",
    "    lora_dropout=0.1, # Dropout is a regularization technique where a proportion of neurons (or parameters) are randomly “dropped out” or turned off during training to prevent overfitting.\n",
    "    bias=\"none\", # Specifies if the bias parameters should be trained. Can be 'none', 'all' or 'lora_only'.\n",
    "    task_type=\"CAUSAL_LM\", # Task to perform, Causal LM: Causal language modeling.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ce4ca6d-883d-4a16-9c07-8ac2fd0b3abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ce47lin/miniconda3/envs/review/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63dd12af2d5340a4b1cfc1dcdd8f94b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Model config\n",
    "# Where the model is placed,set device_map=\"auto\" loads a model onto multiple GPUs..\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bf934d-c277-4857-83bc-925a28109fa0",
   "metadata": {},
   "source": [
    "Note the special characters that are introduced. They are the same that Meta used for the pre-training of the model. They are used to define to the model the different roles and the instructions statements of each role. They can be consulted here: https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3/ (When consulting the link care about both models, Llama 3 and Llama 3 Instruct)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edc3c7bd-5809-4e4d-a974-5ac8041d3daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(prompt):\n",
    "    return f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{prompt['instruction']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n{prompt['output']}<|eot_id|><|end_of_text|>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60a515ac-8b34-46e9-ba2d-3ff777ec826c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    learning_rate=2e-6,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    num_train_epochs=20,\n",
    "    fp16=False,\n",
    "    bf16=True, #bf16 to True with an A100\n",
    "    logging_steps=1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=0.05,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_steps=10,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    report_to=\"wandb\",\n",
    "    output_dir=\"./results/\",\n",
    "    save_strategy='no', # Only safe the final model, not the checkpoints\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e31e0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ce47lin/miniconda3/envs/review/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:246: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    max_seq_length=None,\n",
    "    args=training_arguments,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,# Because the dataset already has the format {\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n",
    "    formatting_func=formatting_prompts_func,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31e1e64f-8e24-4478-abbd-4c6f0bae3cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ce47lin/review/wandb/run-20240516_094834-lr9747ff</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/prem-incar/huggingface/runs/lr9747ff' target=\"_blank\">stoic-serenity-15</a></strong> to <a href='https://wandb.ai/prem-incar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/prem-incar/huggingface' target=\"_blank\">https://wandb.ai/prem-incar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/prem-incar/huggingface/runs/lr9747ff' target=\"_blank\">https://wandb.ai/prem-incar/huggingface/runs/lr9747ff</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='840' max='840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [840/840 10:30, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.310000</td>\n",
       "      <td>1.242012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>1.401000</td>\n",
       "      <td>1.236067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>1.202200</td>\n",
       "      <td>1.229693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>0.948200</td>\n",
       "      <td>1.223773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.115100</td>\n",
       "      <td>1.216029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>252</td>\n",
       "      <td>1.289200</td>\n",
       "      <td>1.209604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>294</td>\n",
       "      <td>1.213200</td>\n",
       "      <td>1.205360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>336</td>\n",
       "      <td>1.112000</td>\n",
       "      <td>1.203412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>378</td>\n",
       "      <td>1.302900</td>\n",
       "      <td>1.202893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>1.292500</td>\n",
       "      <td>1.202379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>462</td>\n",
       "      <td>1.240700</td>\n",
       "      <td>1.202321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>504</td>\n",
       "      <td>1.289000</td>\n",
       "      <td>1.202689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>546</td>\n",
       "      <td>1.358800</td>\n",
       "      <td>1.202217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>588</td>\n",
       "      <td>1.214200</td>\n",
       "      <td>1.202403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>1.254900</td>\n",
       "      <td>1.202196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>672</td>\n",
       "      <td>1.309800</td>\n",
       "      <td>1.202339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>714</td>\n",
       "      <td>1.254100</td>\n",
       "      <td>1.202092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>756</td>\n",
       "      <td>1.188700</td>\n",
       "      <td>1.202071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>798</td>\n",
       "      <td>1.340600</td>\n",
       "      <td>1.202489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>1.369700</td>\n",
       "      <td>1.202071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=840, training_loss=1.2616559531007494, metrics={'train_runtime': 637.4507, 'train_samples_per_second': 2.635, 'train_steps_per_second': 1.318, 'total_flos': 7.774663832764416e+16, 'train_loss': 1.2616559531007494, 'epoch': 20.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fe16068-e588-40f9-8aca-a6a66ae17d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ce47lin/miniconda3/envs/review/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('final_checkpoint/tokenizer_config.json',\n",
       " 'final_checkpoint/special_tokens_map.json',\n",
       " 'final_checkpoint/tokenizer.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model('final_checkpoint')\n",
    "tokenizer.save_pretrained('final_checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e12cc7c-b1c0-442d-9e2c-2f54eb3af53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flush memory\n",
    "del trainer, model\n",
    "gc.collect()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c38eaf24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e619b71a747c451695fe571c37270697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ce47lin/miniconda3/envs/review/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Reload tokenizer and model\n",
    "llama_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a27be4d-6347-4467-b29f-362b96bf4013",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ce47lin/miniconda3/envs/review/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('TunedLlama-3-8B/tokenizer_config.json',\n",
       " 'TunedLlama-3-8B/special_tokens_map.json',\n",
       " 'TunedLlama-3-8B/tokenizer.json')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge adapter with base model\n",
    "sft_model = PeftModel.from_pretrained(llama_model, 'final_checkpoint')\n",
    "sft_model = sft_model.merge_and_unload()\n",
    "\n",
    "# Save model and tokenizer\n",
    "sft_model.save_pretrained(new_model)\n",
    "tokenizer.save_pretrained(new_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
