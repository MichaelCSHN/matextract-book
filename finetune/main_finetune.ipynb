{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "297f6078-0f90-4c3b-92d7-c84428fe6ba8",
   "metadata": {},
   "source": [
    "We start by installing all the packages that are needed for the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98091ffd-b36f-459b-b344-f2dffe18cc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qqq -U torch transformers datasets evaluate accelerate peft trl langchain bitsandbytes tensorboard python-dotenv wandb --progress-bar off"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc6e384-ca74-465f-95e2-283397462739",
   "metadata": {},
   "source": [
    "And import all the packages and functions that are needed. We will do the fine-tuning using the HuggingFace packages that greatly simplifies the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b4b2bce-a836-4f7c-a354-dd877753c0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ[\"WANDB_SILENT\"]=\"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffef89be-1a84-4370-b1f9-72f204c133c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import json\n",
    "import wandb\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import (\n",
    "    LoraConfig, \n",
    "    PeftModel,\n",
    ")\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    ")\n",
    "from trl import (\n",
    "    SFTTrainer,\n",
    "    DataCollatorForCompletionOnlyLM,\n",
    ")\n",
    "from evaluate import load\n",
    "from accelerate import PartialState\n",
    "import langchain\n",
    "from langchain.cache import SQLiteCache\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c057bf-e000-4c08-bd88-51db948440b7",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "It is important to include the *HF_token* in the *.env* file. By the time this notebook was created, the model that we are going to fine-tune (Llama3-8B) is only available after request for access.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2248c01-2cb4-4e34-bcc9-6772832b4c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(\".env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75d4bd2-b956-4240-90bd-4dc67bbddd02",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "This will allow to report the training to Weights and Biases being very easy to consult the loss curve and other interesting parameters of the training.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2814123e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10a8e10a-48aa-4bcb-88be-e27478dc5c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "base_model = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# Name of the new model\n",
    "new_model = \"TunedLlama-3-8B\"\n",
    "\n",
    "# Dataset\n",
    "dataset_path = \"LLM_organic_synthesis/workplace_data/datasets/USPTO-n100k-t2048_exp1/train.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74d8128c-0f3a-4057-85b9-192fdaa10d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_string = PartialState().process_index\n",
    "\n",
    "compute_dtype = getattr(torch, \"float16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83fe6547-e545-4921-a857-88efc9569795",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"json\", data_files=dataset_path, split=\"train\")\n",
    "dataset = dataset.shuffle(seed=42).select(range(1000)) # Only use 5000 samples for quick demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5155090e-f419-4462-a736-eac0fc2d4da6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'output'],\n",
       "        num_rows: 900\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['instruction', 'output'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56084e92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Below is a description of an organic reaction. Extract information from it to an ORD JSON record.\\n\\n### Procedure:\\nTo a solution of 5-bromo-2-methyl-1H-indole (3.0 g, 14.35 mmol) in dry tetrahydrofuran (20 ml) was added sodium hydride (900 mg, 22.5 mmol) with ice-cooling. After stifling for about 30 min, a solution of t-BuLi (27.5 ml, 1.3 M solution in hexane) was added dropwise with stifling at −78° C. under an inert atmosphere of nitrogen. The reaction mixture was warmed slowly to −40° C. over 45 min and stirred at this temperature for another 30 min. The mixture was cooled again below −78° C., followed by the addition of 4,4,5,5-tetramethyl-2-(propan-2-yloxy)-1,3,2-dioxaborolane (5.3 g, 28.49 mmol) dropwise. After warming to room temperature, the mixture was quenched with NH4Cl solution (100 ml) and extracted with ethyl acetate (3×100 ml). The combined organic layers were dried over anhydrous sodium sulfate, filtered and concentrated under reduced pressure to give the residue, which was purified by a silica gel column (2% ethyl acetate in petroleum ether) to afford 2-methyl-5-(4,4,5,5-tetramethyl-1,3,2-dioxaborolan-2-yl)-1H-indole (1.4 g, 38%). 1H NMR (300 MHz, CDCl3): δ 8.06 (s, 1H), 7.91 (s, 1H), 7.58-7.60 (t, J=7.5 Hz, 1H), 7.31 (d, J=8.1 Hz, 1H), 6.24 (s, 1H), 2.46 (s, 3H), 1.39 (s, 12H).\\n\\n### ORD JSON:\\n',\n",
       " 'output': '{\"inputs\": {\"m4\": {\"components\": [{\"identifiers\": [{\"type\": \"NAME\", \"value\": \"4,4,5,5-tetramethyl-2-(propan-2-yloxy)-1,3,2-dioxaborolane\"}], \"amount\": {\"mass\": {\"value\": 5.3, \"units\": \"GRAM\"}}, \"reaction_role\": \"REACTANT\"}]}, \"m1_m5_m2\": {\"components\": [{\"identifiers\": [{\"type\": \"NAME\", \"value\": \"5-bromo-2-methyl-1H-indole\"}], \"amount\": {\"mass\": {\"value\": 3.0, \"units\": \"GRAM\"}}, \"reaction_role\": \"REACTANT\"}, {\"identifiers\": [{\"type\": \"NAME\", \"value\": \"sodium hydride\"}], \"amount\": {\"mass\": {\"value\": 900.0, \"units\": \"MILLIGRAM\"}}, \"reaction_role\": \"REACTANT\"}, {\"identifiers\": [{\"type\": \"NAME\", \"value\": \"tetrahydrofuran\"}], \"amount\": {\"volume\": {\"value\": 20.0, \"units\": \"MILLILITER\"}}, \"reaction_role\": \"SOLVENT\"}]}, \"m3\": {\"components\": [{\"identifiers\": [{\"type\": \"NAME\", \"value\": \"t-BuLi\"}], \"amount\": {\"volume\": {\"value\": 27.5, \"units\": \"MILLILITER\"}}, \"reaction_role\": \"REACTANT\"}]}}, \"conditions\": {\"temperature\": {\"setpoint\": {\"value\": -40.0, \"units\": \"CELSIUS\"}}, \"stirring\": {\"type\": \"CUSTOM\", \"details\": \"stirred at this temperature for another 30 min\"}, \"conditions_are_dynamic\": true}, \"workups\": [{\"type\": \"TEMPERATURE\", \"details\": \"cooling\"}, {\"type\": \"CUSTOM\", \"details\": \"with stifling at \\\\u221278\\\\u00b0 C. under an inert atmosphere of nitrogen\", \"temperature\": {\"setpoint\": {\"value\": -78.0, \"units\": \"CELSIUS\"}}}, {\"type\": \"TEMPERATURE\", \"details\": \"The mixture was cooled again below \\\\u221278\\\\u00b0 C.\"}, {\"type\": \"TEMPERATURE\", \"details\": \"After warming to room temperature\", \"temperature\": {\"control\": {\"type\": \"AMBIENT\"}}}, {\"type\": \"CUSTOM\", \"details\": \"the mixture was quenched with NH4Cl solution (100 ml)\", \"input\": {\"components\": [{\"identifiers\": [{\"type\": \"NAME\", \"value\": \"NH4Cl\"}], \"amount\": {\"volume\": {\"value\": 100.0, \"units\": \"MILLILITER\"}}, \"reaction_role\": \"WORKUP\"}]}}, {\"type\": \"EXTRACTION\", \"details\": \"extracted with ethyl acetate (3\\\\u00d7100 ml)\", \"input\": {\"components\": [{\"identifiers\": [{\"type\": \"NAME\", \"value\": \"ethyl acetate\"}], \"amount\": {\"volume\": {\"value\": 100.0, \"units\": \"MILLILITER\"}}, \"reaction_role\": \"WORKUP\"}]}}, {\"type\": \"DRY_WITH_MATERIAL\", \"details\": \"The combined organic layers were dried over anhydrous sodium sulfate\", \"input\": {\"components\": [{\"identifiers\": [{\"type\": \"NAME\", \"value\": \"sodium sulfate\"}], \"reaction_role\": \"WORKUP\"}]}}, {\"type\": \"FILTRATION\", \"details\": \"filtered\"}, {\"type\": \"CONCENTRATION\", \"details\": \"concentrated under reduced pressure\"}, {\"type\": \"CUSTOM\", \"details\": \"to give the residue, which\"}, {\"type\": \"CUSTOM\", \"details\": \"was purified by a silica gel column (2% ethyl acetate in petroleum ether)\", \"input\": {\"components\": [{\"identifiers\": [{\"type\": \"NAME\", \"value\": \"ethyl acetate\"}], \"reaction_role\": \"WORKUP\"}, {\"identifiers\": [{\"type\": \"NAME\", \"value\": \"petroleum ether\"}], \"reaction_role\": \"WORKUP\"}]}}], \"outcomes\": [{\"reaction_time\": {\"value\": 30.0, \"units\": \"MINUTE\"}, \"products\": [{\"identifiers\": [{\"type\": \"NAME\", \"value\": \"2-methyl-5-(4,4,5,5-tetramethyl-1,3,2-dioxaborolan-2-yl)-1H-indole\"}], \"measurements\": [{\"type\": \"AMOUNT\", \"details\": \"MASS\", \"amount\": {\"mass\": {\"value\": 1.4, \"units\": \"GRAM\"}}}, {\"type\": \"YIELD\", \"details\": \"PERCENTYIELD\", \"percentage\": {\"value\": 38.0}}], \"reaction_role\": \"PRODUCT\"}]}]}'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a155007-573c-47f7-a8cc-3a3d136b5b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QLoRA config\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "# Compute dtype for 4-bit base models\n",
    "# Quantization type (fp4 or nf4)\n",
    "# Activate nested quantization for 4-bit base models\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# LoRA config\n",
    "peft_config = LoraConfig(\n",
    "    r=32, # The rank of the update matrices, expressed in int. Lower rank results in smaller update matrices with fewer trainable parameters.\n",
    "    lora_alpha=64, # LoRA scaling factor. It changes how the adaptation layer's weights affect the base model's\n",
    "    lora_dropout=0.1, # Dropout is a regularization technique where a proportion of neurons (or parameters) are randomly “dropped out” or turned off during training to prevent overfitting.\n",
    "    bias=\"none\", # Specifies if the bias parameters should be trained. Can be 'none', 'all' or 'lora_only'.\n",
    "    task_type=\"CAUSAL_LM\", # Task to perform, Causal LM: Causal language modeling.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ce4ca6d-883d-4a16-9c07-8ac2fd0b3abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa8a5d3fb0c24a12b55036eb75397f42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Model config\n",
    "# Where the model is placed,set device_map=\"auto\" loads a model onto multiple GPUs..\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edc3c7bd-5809-4e4d-a974-5ac8041d3daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example['instruction'])):\n",
    "        text = f\"### Question: {example['instruction'][i]}\\n ### Answer: {example['output'][i]}\"\n",
    "        output_texts.append(text)\n",
    "    return output_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bf934d-c277-4857-83bc-925a28109fa0",
   "metadata": {},
   "source": [
    "Note the special characters that are introduced. They are the same that Meta used for the pre-training of the model. They are used to define to the model the different roles and the instructions statements of each role. They can be consulted here: https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3/ \n",
    "\n",
    "```{margin}\n",
    "When consulting the link care about both models, Llama-3 and Llama-3 Instruct. In this Notebook, the Instruct version is the one being used.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60a515ac-8b34-46e9-ba2d-3ff777ec826c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    learning_rate=3e-6,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    num_train_epochs=10,\n",
    "    fp16=False,\n",
    "    bf16=True, #bf16 to True with an A100\n",
    "    logging_steps=1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=0.05,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_steps=10,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    output_dir=\"./results/\",\n",
    "    save_strategy='no', # Only safe the final model, not the checkpoints\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5eae84a-4db0-44e2-8d1d-89a46f94c434",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_template = \" ### Answer:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16b8e16a-f753-4e32-9b8d-946ba640c943",
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e31e0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    max_seq_length=None,\n",
    "    args=training_arguments,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=False,\n",
    "    formatting_func=formatting_prompts_func,\n",
    "    data_collator=collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31e1e64f-8e24-4478-abbd-4c6f0bae3cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='560' max='560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [560/560 39:54, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.999600</td>\n",
       "      <td>0.636476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.836100</td>\n",
       "      <td>0.604613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.827700</td>\n",
       "      <td>0.574966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.753000</td>\n",
       "      <td>0.549391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.683700</td>\n",
       "      <td>0.527555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>0.700800</td>\n",
       "      <td>0.507865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>0.713200</td>\n",
       "      <td>0.489182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.599200</td>\n",
       "      <td>0.471585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>252</td>\n",
       "      <td>0.643600</td>\n",
       "      <td>0.460574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.603600</td>\n",
       "      <td>0.455087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>308</td>\n",
       "      <td>0.593000</td>\n",
       "      <td>0.452704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>336</td>\n",
       "      <td>0.609400</td>\n",
       "      <td>0.451262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>364</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.450476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>392</td>\n",
       "      <td>0.619100</td>\n",
       "      <td>0.449996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.587200</td>\n",
       "      <td>0.449747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>448</td>\n",
       "      <td>0.620800</td>\n",
       "      <td>0.449778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>476</td>\n",
       "      <td>0.550800</td>\n",
       "      <td>0.449613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>504</td>\n",
       "      <td>0.577700</td>\n",
       "      <td>0.449670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>532</td>\n",
       "      <td>0.564000</td>\n",
       "      <td>0.449629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.576700</td>\n",
       "      <td>0.449578</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=560, training_loss=0.5340924625418015, metrics={'train_runtime': 2403.4626, 'train_samples_per_second': 3.745, 'train_steps_per_second': 0.233, 'total_flos': 3.342546165492941e+17, 'train_loss': 0.5340924625418015, 'epoch': 9.955555555555556})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0c88664-598e-462c-991a-e207678de7c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'output'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds_path = \"test.json\"\n",
    "test_dataset = load_dataset(\"json\", data_files=test_ds_path, split=\"train\")\n",
    "test_dataset = test_dataset.shuffle(seed=42)\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7b60d2d-5f2c-4591-9ae2-dc205ca16b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "sft_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    temperature=0.01,\n",
    "    model=trainer.model,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1e5c9e4-b7fe-41a0-a792-6945010fcacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX = \"\"\"<|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful scientific assistant. Your task is to extract information about organic reactions. {shot}\"\"\"\n",
    "SUFFIX = \"\"\"<|start_header_id|>user<|end_header_id|>\\n\\n{sample}<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\"\"\n",
    "SHOT = \"\"\"\n",
    "One example is provided to you to show how to perform the task:\n",
    "\n",
    "### Procedure:\\nA suspension of 8 g of the product of Example 7 and 0.4 g of DABCO in 90 ml of xylenes were heated under N2 at 130\\u00b0-135\\u00b0 C. while 1.8 ml of phosgene was added portionwise at a rate to maintain a reflux temperature of about 130\\u00b0-135\\u00b0 C. The mixture was refluxed an additional two hours, cooled under N2 to room temperature, filtered, and the filtrate was concentrated in vacuo to yield 6.9 g of the subject compound as a crude oil.\\n\\n\n",
    "### ORD JSON:\\n{\\\"inputs\\\": {\\\"m1_m2_m4\\\": {\\\"components\\\": [{\\\"identifiers\\\": [{\\\"type\\\": \\\"NAME\\\", \\\"value\\\": \\\"product\\\"}], \\\"amount\\\": {\\\"mass\\\": {\\\"value\\\": 8.0, \\\"units\\\": \\\"GRAM\\\"}}, \\\"reaction_role\\\": \\\"REACTANT\\\"}, {\\\"identifiers\\\": [{\\\"type\\\": \\\"NAME\\\", \\\"value\\\": \\\"DABCO\\\"}], \\\"amount\\\": {\\\"mass\\\": {\\\"value\\\": 0.4, \\\"units\\\": \\\"GRAM\\\"}}, \\\"reaction_role\\\": \\\"REACTANT\\\"}, {\\\"identifiers\\\": [{\\\"type\\\": \\\"NAME\\\", \\\"value\\\": \\\"xylenes\\\"}], \\\"amount\\\": {\\\"volume\\\": {\\\"value\\\": 90.0, \\\"units\\\": \\\"MILLILITER\\\"}}, \\\"reaction_role\\\": \\\"SOLVENT\\\"}]}, \\\"m3\\\": {\\\"components\\\": [{\\\"identifiers\\\": [{\\\"type\\\": \\\"NAME\\\", \\\"value\\\": \\\"phosgene\\\"}], \\\"amount\\\": {\\\"volume\\\": {\\\"value\\\": 1.8, \\\"units\\\": \\\"MILLILITER\\\"}}, \\\"reaction_role\\\": \\\"REACTANT\\\"}]}}, \\\"conditions\\\": {\\\"temperature\\\": {\\\"control\\\": {\\\"type\\\": \\\"AMBIENT\\\"}}, \\\"conditions_are_dynamic\\\": true}, \\\"workups\\\": [{\\\"type\\\": \\\"ADDITION\\\", \\\"details\\\": \\\"was added portionwise at a rate\\\"}, {\\\"type\\\": \\\"TEMPERATURE\\\", \\\"details\\\": \\\"to maintain a reflux temperature of about 130\\\\u00b0-135\\\\u00b0 C\\\"}, {\\\"type\\\": \\\"TEMPERATURE\\\", \\\"details\\\": \\\"The mixture was refluxed an additional two hours\\\", \\\"duration\\\": {\\\"value\\\": 2.0, \\\"units\\\": \\\"HOUR\\\"}}, {\\\"type\\\": \\\"FILTRATION\\\", \\\"details\\\": \\\"filtered\\\"}, {\\\"type\\\": \\\"CONCENTRATION\\\", \\\"details\\\": \\\"the filtrate was concentrated in vacuo\\\"}], \\\"outcomes\\\": [{\\\"products\\\": [{\\\"identifiers\\\": [{\\\"type\\\": \\\"NAME\\\", \\\"value\\\": \\\"subject compound\\\"}], \\\"measurements\\\": [{\\\"type\\\": \\\"AMOUNT\\\", \\\"details\\\": \\\"MASS\\\", \\\"amount\\\": {\\\"mass\\\": {\\\"value\\\": 6.9, \\\"units\\\": \\\"GRAM\\\"}}}], \\\"reaction_role\\\": \\\"PRODUCT\\\"}]}]}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f645766-be90-4d68-ad8d-304d69b9ba4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working in the 0-shot prompts\n",
      "Working in the prompt 1\n",
      "Working in the prompt 2\n",
      "Working in the prompt 3\n",
      "Working in the prompt 4\n",
      "Working in the prompt 5\n",
      "Working in the prompt 6\n",
      "Working in the prompt 7\n",
      "Working in the prompt 8\n",
      "Working in the prompt 9\n",
      "Working in the prompt 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working in the prompt 11\n",
      "Working in the prompt 12\n",
      "Working in the prompt 13\n",
      "Working in the prompt 14\n",
      "Working in the prompt 15\n",
      "Working in the prompt 16\n",
      "Working in the prompt 17\n",
      "Working in the prompt 18\n",
      "Working in the prompt 19\n",
      "Working in the prompt 20\n",
      "Working in the prompt 21\n",
      "Working in the prompt 22\n",
      "Working in the prompt 23\n",
      "Working in the prompt 24\n",
      "Working in the prompt 25\n",
      "Working in the prompt 26\n",
      "Working in the prompt 27\n",
      "Working in the prompt 28\n",
      "Working in the prompt 29\n",
      "Working in the prompt 30\n",
      "Working in the prompt 31\n",
      "Working in the prompt 32\n",
      "Working in the prompt 33\n",
      "Working in the prompt 34\n",
      "Working in the prompt 35\n",
      "Working in the prompt 36\n",
      "Working in the prompt 37\n",
      "Working in the prompt 38\n",
      "Working in the prompt 39\n",
      "Working in the prompt 40\n",
      "Working in the prompt 41\n",
      "Working in the prompt 42\n",
      "Working in the prompt 43\n",
      "Working in the prompt 44\n",
      "Working in the prompt 45\n",
      "Working in the prompt 46\n",
      "Working in the prompt 47\n",
      "Working in the prompt 48\n",
      "Working in the prompt 49\n",
      "Working in the prompt 50\n",
      "Working in the prompt 51\n",
      "Working in the prompt 52\n",
      "Working in the prompt 53\n",
      "Working in the prompt 54\n",
      "Working in the prompt 55\n",
      "Working in the prompt 56\n",
      "Working in the prompt 57\n",
      "Working in the prompt 58\n",
      "Working in the prompt 59\n",
      "Working in the prompt 60\n",
      "Working in the prompt 61\n",
      "Working in the prompt 62\n",
      "Working in the prompt 63\n",
      "Working in the prompt 64\n",
      "Working in the prompt 65\n",
      "Working in the prompt 66\n",
      "Working in the prompt 67\n",
      "Working in the prompt 68\n",
      "Working in the prompt 69\n",
      "Working in the prompt 70\n",
      "Working in the prompt 71\n",
      "Working in the prompt 72\n",
      "Working in the prompt 73\n",
      "Working in the prompt 74\n",
      "Working in the prompt 75\n",
      "Working in the prompt 76\n",
      "Working in the prompt 77\n",
      "Working in the prompt 78\n",
      "Working in the prompt 79\n",
      "Working in the prompt 80\n",
      "Working in the prompt 81\n",
      "Working in the prompt 82\n",
      "Working in the prompt 83\n",
      "Working in the prompt 84\n",
      "Working in the prompt 85\n",
      "Working in the prompt 86\n",
      "Working in the prompt 87\n",
      "Working in the prompt 88\n",
      "Working in the prompt 89\n",
      "Working in the prompt 90\n",
      "Working in the prompt 91\n",
      "Working in the prompt 92\n",
      "Working in the prompt 93\n",
      "Working in the prompt 94\n",
      "Working in the prompt 95\n",
      "Working in the prompt 96\n",
      "Working in the prompt 97\n",
      "Working in the prompt 98\n",
      "Working in the prompt 99\n",
      "Working in the prompt 100\n",
      "Working in the 1-shot prompts\n",
      "Working in the prompt 1\n",
      "Working in the prompt 2\n",
      "Working in the prompt 3\n",
      "Working in the prompt 4\n",
      "Working in the prompt 5\n",
      "Working in the prompt 6\n",
      "Working in the prompt 7\n",
      "Working in the prompt 8\n",
      "Working in the prompt 9\n",
      "Working in the prompt 10\n",
      "Working in the prompt 11\n",
      "Working in the prompt 12\n",
      "Working in the prompt 13\n",
      "Working in the prompt 14\n",
      "Working in the prompt 15\n",
      "Working in the prompt 16\n",
      "Working in the prompt 17\n",
      "Working in the prompt 18\n",
      "Working in the prompt 19\n",
      "Working in the prompt 20\n",
      "Working in the prompt 21\n",
      "Working in the prompt 22\n",
      "Working in the prompt 23\n",
      "Working in the prompt 24\n",
      "Working in the prompt 25\n",
      "Working in the prompt 26\n",
      "Working in the prompt 27\n",
      "Working in the prompt 28\n",
      "Working in the prompt 29\n",
      "Working in the prompt 30\n",
      "Working in the prompt 31\n",
      "Working in the prompt 32\n",
      "Working in the prompt 33\n",
      "Working in the prompt 34\n",
      "Working in the prompt 35\n",
      "Working in the prompt 36\n",
      "Working in the prompt 37\n",
      "Working in the prompt 38\n",
      "Working in the prompt 39\n",
      "Working in the prompt 40\n",
      "Working in the prompt 41\n",
      "Working in the prompt 42\n",
      "Working in the prompt 43\n",
      "Working in the prompt 44\n",
      "Working in the prompt 45\n",
      "Working in the prompt 46\n",
      "Working in the prompt 47\n",
      "Working in the prompt 48\n",
      "Working in the prompt 49\n",
      "Working in the prompt 50\n",
      "Working in the prompt 51\n",
      "Working in the prompt 52\n",
      "Working in the prompt 53\n",
      "Working in the prompt 54\n",
      "Working in the prompt 55\n",
      "Working in the prompt 56\n",
      "Working in the prompt 57\n",
      "Working in the prompt 58\n",
      "Working in the prompt 59\n",
      "Working in the prompt 60\n",
      "Working in the prompt 61\n",
      "Working in the prompt 62\n",
      "Working in the prompt 63\n",
      "Working in the prompt 64\n",
      "Working in the prompt 65\n",
      "Working in the prompt 66\n",
      "Working in the prompt 67\n",
      "Working in the prompt 68\n",
      "Working in the prompt 69\n",
      "Working in the prompt 70\n",
      "Working in the prompt 71\n",
      "Working in the prompt 72\n",
      "Working in the prompt 73\n",
      "Working in the prompt 74\n",
      "Working in the prompt 75\n",
      "Working in the prompt 76\n",
      "Working in the prompt 77\n",
      "Working in the prompt 78\n",
      "Working in the prompt 79\n",
      "Working in the prompt 80\n",
      "Working in the prompt 81\n",
      "Working in the prompt 82\n",
      "Working in the prompt 83\n",
      "Working in the prompt 84\n",
      "Working in the prompt 85\n",
      "Working in the prompt 86\n",
      "Working in the prompt 87\n",
      "Working in the prompt 88\n",
      "Working in the prompt 89\n",
      "Working in the prompt 90\n",
      "Working in the prompt 91\n",
      "Working in the prompt 92\n",
      "Working in the prompt 93\n",
      "Working in the prompt 94\n",
      "Working in the prompt 95\n",
      "Working in the prompt 96\n",
      "Working in the prompt 97\n",
      "Working in the prompt 98\n",
      "Working in the prompt 99\n",
      "Working in the prompt 100\n"
     ]
    }
   ],
   "source": [
    "# Generate text for the 0-shot\n",
    "results = {}\n",
    "for i in range(2):\n",
    "    print(f\"Working in the {i}-shot prompts\")\n",
    "    references = []\n",
    "    predictions_sft = []\n",
    "    prompts = []\n",
    "    count = 0\n",
    "    for t in test_dataset:\n",
    "        count += 1\n",
    "        print(f\"Working in the prompt {count}\")\n",
    "        instruction = t['instruction']\n",
    "        output = t['output']\n",
    "        if i == 0:\n",
    "            shot = ''\n",
    "        else:\n",
    "            shot = SHOT\n",
    "        system = PREFIX.format(shot=shot)\n",
    "        user = SUFFIX.format(sample=instruction)\n",
    "        prompt = system + user\n",
    "        references.append(output)\n",
    "        \n",
    "        with torch.cuda.amp.autocast():\n",
    "            pred = sft_pipe(prompt)\n",
    "        predictions_sft.append(pred[0]['generated_text'].replace(prompt, ''))\n",
    "    \n",
    "    results [f\"{i}-shot\"] = {\n",
    "        \"predictions\": predictions_sft,\n",
    "        \"references\": references,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9bc12286-4cbe-4185-8a76-aa96c16fd6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bertscore = load(\"bertscore\")\n",
    "for i in range(2):\n",
    "    predictions_sft = results[f'{i}-shot'][\"predictions\"]\n",
    "    references = results[f'{i}-shot'][\"references\"]\n",
    "\n",
    "    results_sft = bertscore.compute(predictions=predictions_sft, references=references, model_type=\"distilbert-base-uncased\")\n",
    "\n",
    "    results[f\"{i}-shot\"].update({\n",
    "        \"precision\": mean(results_sft[\"precision\"]),\n",
    "        \"recall\": mean(results_sft[\"recall\"]),\n",
    "        \"f1_scores\": mean(results_sft[\"f1\"]),\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6baef183-ded2-4b9a-a3ef-22531b36d20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sft_results.json', 'w') as f:\n",
    "   json.dump(results, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
