{
 "cells": [
  {
   "cell_type": "raw",
   "source": [
    "{#sec-crossref}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a65f785e20c160e1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Obtaining a set of relevant data sources\n",
    "\n",
    "At the start of the data extraction process you have to collect a set of potentially relevant data sources. Therefore, you could collect a dataset manually or use a tool to help automating and speeding up this process. The Crossref API is a very useful tool to collect the metadata of relevant articles. Besides the API there are multiple Python libraries available that make access to the API easier. One of these libraries is [crossrefapi](https://github.com/fabiobatalha/crossrefapi). As an example 100 sources including metadata on the topic 'buchwald-hartwig coupling' are extracted and saved into a json file. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "626148e6d1522938"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from crossref.restful import Works\n",
    "import json\n",
    "\n",
    "works = Works(timeout=60)\n",
    "\n",
    "# Performing the search for sources on the topic of buchwald-hartwig coupling for 10 papers\n",
    "query_result = works.query(bibliographic='buchwald-hartwig coupling').select('DOI', 'title', 'author', 'type', 'publisher', 'issued').sample(10)\n",
    "\n",
    "results = [item for item in query_result]\n",
    "\n",
    "# Save 100 results including their metadata in a json file\n",
    "with open('buchwald-hartwig_coupling_results.json', 'w') as file:\n",
    "    json.dump(results, file)\n",
    "    \n",
    "print(results)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf4c4bfd1ae1b6e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "With the obtained metadata one could afterwards try to filter for relevant or available data sources which could be downloaded through an API provided by the publishers or by data corpus. Some of these APIs provide also a filter for the download of articles, do one could insert the received metadata in the API to download the relevant articles. An example for using such an article downloading API is provided in [Chapter @data_mining]."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "903606d800c9c0df"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
