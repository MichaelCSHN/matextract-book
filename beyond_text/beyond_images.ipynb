{
 "cells": [
  {
   "cell_type": "raw",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "dabfd37599ae03a1"
  },
  {
   "cell_type": "raw",
   "source": [
    "---\n",
    "author: Mara Wilhelmi\n",
    "date: 2024-05-24\n",
    "title: 7 | Beyond text\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da701e21fecca2a0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "NLP-LLMs tend to have problems with analysing and understanding complex structures such as tables, plots and images included in scientific articles. Since especially in chemistry and material science information about chemical components is included in these, one should think about different approaches for these structures. Therefore, vision language models (VLMs) since they can analyse images alongside text. There are several open and closed-source VLMs available e.g. [Vision models from OpenAI](https://platform.openai.com/docs/guides/vision), [Claude models](https://docs.anthropic.com/en/docs/vision) and [DeepSeek-VL](https://github.com/deepseek-ai/DeepSeek-VL). As an example the extraction of images with [GPT4-o](https://platform.openai.com/docs/models/gpt-4o) is shown:\n",
    "\n",
    "First one has to convert the file into images. As an example a PDF file obtained in the [Section 1](../obtaining_data/data_mining.ipynb) is converted into images."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f3a5e72798406fa1"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path\n",
    "\n",
    "file_path = '../obtaining_data/PDFs/10.26434_chemrxiv-2024-1l0sn.pdf'\n",
    "\n",
    "# converting the PDF files to images \n",
    "pdf_images = convert_from_path(file_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-28T07:54:02.942697Z",
     "start_time": "2024-05-28T07:54:02.547049Z"
    }
   },
   "id": "b886d18ec7e86797"
  },
  {
   "cell_type": "markdown",
   "source": [
    "After that one should process the obtained images fo instance rotate pages with vertical text since many models have problems with this. Next one has to convert the pictures into machine-readable Base-64 format. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b22d2d2c714687fd"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maraw/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] ../obtaining_data/PDFs/10.26434_chemrxiv-2024-1l0sn.pdf - corrected image saved as ./images/corrected_10.26434_chemrxiv-2024-1l0sn_page1.png\n",
      "[INFO] ../obtaining_data/PDFs/10.26434_chemrxiv-2024-1l0sn.pdf - corrected image saved as ./images/corrected_10.26434_chemrxiv-2024-1l0sn_page2.png\n",
      "[INFO] ../obtaining_data/PDFs/10.26434_chemrxiv-2024-1l0sn.pdf - corrected image saved as ./images/corrected_10.26434_chemrxiv-2024-1l0sn_page3.png\n",
      "[INFO] ../obtaining_data/PDFs/10.26434_chemrxiv-2024-1l0sn.pdf - corrected image saved as ./images/corrected_10.26434_chemrxiv-2024-1l0sn_page4.png\n",
      "[INFO] ../obtaining_data/PDFs/10.26434_chemrxiv-2024-1l0sn.pdf - corrected image saved as ./images/corrected_10.26434_chemrxiv-2024-1l0sn_page5.png\n",
      "[INFO] ../obtaining_data/PDFs/10.26434_chemrxiv-2024-1l0sn.pdf - corrected image saved as ./images/corrected_10.26434_chemrxiv-2024-1l0sn_page6.png\n"
     ]
    }
   ],
   "source": [
    "from pytesseract import Output\n",
    "import pytesseract\n",
    "import imutils\n",
    "import cv2\n",
    "import os\n",
    "import base64\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# process the images to a unified and for an VLM better suiting format\n",
    "def process_image(image, max_size, output_folder, file_path, i):\n",
    "    width, height = image.size\n",
    "    resized_image = resize_image(image, max_size)\n",
    "    rotate_image = correct_text_orientation(resized_image, output_folder, file_path, i)\n",
    "    jpeg_image = convert_to_jpeg(rotate_image)\n",
    "    base64_encoded_image = base64.b64encode(jpeg_image).decode(\"utf-8\")\n",
    "    return (\n",
    "        base64_encoded_image,\n",
    "        max(width, height), \n",
    "    )\n",
    "\n",
    "# most VLM models struggle with rotated text therefore, rotated text gets detect and the pages flipped\n",
    "def correct_text_orientation(image, save_directory, file_path, i):\n",
    "    if isinstance(image, Image.Image):\n",
    "        image = pil_to_cv2(image)\n",
    "\n",
    "    rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = pytesseract.image_to_osd(rgb, output_type=Output.DICT)\n",
    "\n",
    "    rotated = imutils.rotate_bound(image, angle=results[\"rotate\"])\n",
    "\n",
    "    base_filename = os.path.basename(file_path)\n",
    "    name_without_ext, _ = os.path.splitext(base_filename)\n",
    "    new_filename = os.path.join(\n",
    "        save_directory, f\"corrected_{name_without_ext}_page{i+1}.png\"\n",
    "    )\n",
    "\n",
    "    cv2.imwrite(new_filename, rotated)\n",
    "    print(f\"[INFO] {file_path} - corrected image saved as {new_filename}\")\n",
    "    return rotated\n",
    "\n",
    "# the images get converted into jpeg format\n",
    "def convert_to_jpeg(cv2_image):\n",
    "    retval, buffer = cv2.imencode(\".jpg\", cv2_image)\n",
    "    if retval:\n",
    "        return buffer\n",
    "\n",
    "# conversion of the images from a python-image-library object to an OpenCV object\n",
    "def pil_to_cv2(image):\n",
    "    np_image = np.array(image)\n",
    "    cv2_image = cv2.cvtColor(np_image, cv2.COLOR_RGB2BGR)\n",
    "    return cv2_image\n",
    "\n",
    "# the images get resized to a unified size with a maximum dimensions\n",
    "def resize_image(image, max_dimension):\n",
    "    width, height = image.size\n",
    "\n",
    "    # Check if the image has a palette and convert it to true color mode\n",
    "    if image.mode == \"P\":\n",
    "        if \"transparency\" in image.info:\n",
    "            image = image.convert(\"RGBA\")\n",
    "        else:\n",
    "            image = image.convert(\"RGB\")\n",
    "    # convert to black and white\n",
    "    image = image.convert(\"L\")\n",
    "\n",
    "    if width > max_dimension or height > max_dimension:\n",
    "        if width > height:\n",
    "            new_width = max_dimension\n",
    "            new_height = int(height * (max_dimension / width))\n",
    "        else:\n",
    "            new_height = max_dimension\n",
    "            new_width = int(width * (max_dimension / height))\n",
    "        image = image.resize((new_width, new_height), Image.LANCZOS)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "output_folder_images = './images'\n",
    "\n",
    "# all images get preprocessed\n",
    "images_base64 = [process_image(image, 2048, output_folder_images, file_path, j)[0] for j, image in enumerate(pdf_images)]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-28T07:54:08.090636Z",
     "start_time": "2024-05-28T07:54:02.944862Z"
    }
   },
   "id": "19b39a0896010c5f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "As a next step one could call the OpenAI API. Therefore, one needs an API-key to pay for the calls. Moreover, one needs to create the prompt including the images and the text prompt. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fd238c06473113b7"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# the text prompt text for the model call gets defined\n",
    "prompt_text = 'Extract all the relevant information about Buchwald-Hartwig reactions included in these images.'\n",
    "\n",
    "# the composite prompt is put together \n",
    "def get_prompt_vision_model(images_base64, prompt_text):\n",
    "    content = []\n",
    "    # the images get added in base64 format and in the end the text prompt will be added\n",
    "    for data in images_base64:\n",
    "        content.append(create_image_content(data))\n",
    "\n",
    "    content.append({\"type\": \"text\", \"text\": prompt_text})\n",
    "    return content\n",
    "\n",
    "# the images get converted into base64 format\n",
    "def create_image_content(image, detail=\"high\"):\n",
    "    return {\n",
    "        \"type\": \"image_url\",\n",
    "        # the level of detail is set to 'high' since mostly text on the images is small\n",
    "        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\", \"detail\": detail},\n",
    "    }\n",
    "\n",
    "# the composite prompt for the model call gets defined \n",
    "prompt = get_prompt_vision_model(images_base64, prompt_text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-28T07:54:08.094290Z",
     "start_time": "2024-05-28T07:54:08.092567Z"
    }
   },
   "id": "f95a25d2099f080d"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  {\n",
      "  \"Buchwald-Hartwig Reaction Conditions\": [\n",
      "    {\n",
      "      \"Reaction Type\": \"Cross-coupling\",\n",
      "      \"Key Step\": \"Reaction of an α-amino-BODIPY and the respective halide\",\n",
      "      \"Catalyst\": \"Pd(OAc)2\",\n",
      "      \"Ligand\": \"(±)-BINAP\",\n",
      "      \"Base\": \"Cs2CO3\",\n",
      "      \"Solvent\": \"PhMe\",\n",
      "      \"Temperature\": \"80 °C\",\n",
      "      \"Time\": \"1.5 h\",\n",
      "      \"Yield\": \"Up to 68% for unsymmetric BODIPYs\"\n",
      "    },\n",
      "    {\n",
      "      \"Monomer\": \"α-chloro- and α-amino-BODIPYs\",\n",
      "      \"Catalyst\": \"Pd(OAc)2\",\n",
      "      \"Ligand\": \"(±)-BINAP\",\n",
      "      \"Base\": \"Cs2CO3\",\n",
      "      \"Solvent\": \"PhMe\",\n",
      "      \"Temperature\": \"80 °C\",\n",
      "      \"Yield\": \"Up to 68% for unsymmetric BODIPYs\"\n",
      "    },\n",
      "    {\n",
      "      \"Monomer\": \"Br-Ar-mono-NH2\",\n",
      "      \"Catalyst\": \"Pd(OAc)2\",\n",
      "      \"Ligand\": \"(±)-BINAP\",\n",
      "      \"Base\": \"Cs2CO3\",\n",
      "      \"Solvent\": \"PhMe\",\n",
      "      \"Temperature\": \"80 °C\",\n",
      "      \"Time\": \"1.5 h\",\n",
      "      \"Yield\": \"Up to 68% for unsymmetric BODIPYs\"\n",
      "    }\n",
      "  ],\n",
      "  \"Additional Notes\": [\n",
      "    {\n",
      "      \"Note\": \"The reaction showed a trend of improvement with increasing level of substitution of the BODIPY core.\"\n",
      "    },\n",
      "    {\n",
      "      \"Note\": \"The reaction of Br-Ar-mono-Br with EDM-Ar-mono-Br required slow addition of Br-Ar-mono-NH2 to a heated solution of the remaining reagents.\"\n",
      "    },\n",
      "    {\n",
      "      \"Note\": \"The reaction yielded 44% of the functionalized dimer, while 45% of the starting material was recovered.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "Input-token used: 6706  Output_token used:  436\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# the openai api gets called; the temperature is set to 0 since the output should have a high accuracy\n",
    "# the gpt4-o model is used since this is the cheapest and fastest openai vision model\n",
    "def call_openai(\n",
    "    prompt, model=\"gpt-4o\", temperature: float = 0.0, **kwargs\n",
    "):\n",
    "    \"\"\"Call chat openai model\n",
    "\n",
    "    Args:\n",
    "        prompt (str): Prompt to send to model\n",
    "        model (str, optional): Name of the API. Defaults to \"\"gpt-4-vision-preview\".\n",
    "        temperature (float, optional): inference temperature. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        dict: new data\n",
    "    \"\"\"\n",
    "    client = OpenAI()\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a scientific assistant, extracting important information about polymerization conditions\"\n",
    "                \"out of pdfs in valid json format. Extract just data which you are 100% confident about the \"\n",
    "                \"accuracy. Keep the entries short without details. Be careful with numbers.\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        **kwargs,\n",
    "    )\n",
    "    # the input and output token are reported in order to track costs of the api calls\n",
    "    input_tokens = completion.usage.prompt_tokens\n",
    "    output_token = completion.usage.completion_tokens\n",
    "    # the output of the model call is saved\n",
    "    message_content = completion.choices[0].message.content\n",
    "    return message_content, input_tokens, output_token\n",
    "\n",
    "# the openai api key is loading\n",
    "dotenv_path = '../OPENAI_KEY.env'\n",
    "load_dotenv(dotenv_path)\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# the openai api is called and the used token and the output printed\n",
    "output, input_token, output_token = call_openai(prompt=prompt)\n",
    "print('Output: ', output)\n",
    "print('Input-token used:', input_token, ' Output_token used: ', output_token)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-28T07:55:06.274592Z",
     "start_time": "2024-05-28T07:54:49.753036Z"
    }
   },
   "id": "3d37e685fef815ab"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now one could use this structured output to build up a database of Buchwald-Hartwig-Coupling reactions. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "22d3e84b10a7ecba"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
